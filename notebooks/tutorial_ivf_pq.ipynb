{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAFT IVF-PQ tutorial\n",
    "In this tutorial you will learn to build IVF-PQ index and use it to search approximate nearest neighbors (ANN)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cupy as cp\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The index and data will be saved in /tmp/raft_ivf_pq_tutorial\n"
     ]
    }
   ],
   "source": [
    "# We'll need to load store some data in this tutorial\n",
    "import os\n",
    "import tempfile\n",
    "\n",
    "WORK_FOLDER = os.path.join(tempfile.gettempdir(), 'raft_ivf_pq_tutorial')\n",
    "\n",
    "if not os.path.exists(WORK_FOLDER):\n",
    "   os.makedirs(WORK_FOLDER)\n",
    "print(\"The index and data will be saved in\", WORK_FOLDER)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the data\n",
    "We're going to use the data from [ANN benchmarks website](ann-benchmarks.com),\n",
    "which provides a few datasets in [HDF5 format](https://www.hdfgroup.org/solutions/hdf5/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_URL = \"http://ann-benchmarks.com/sift-128-euclidean.hdf5\"\n",
    "DATASET_FILENAME = DATASET_URL.split('/')[-1]\n",
    "\n",
    "## download the dataset\n",
    "import urllib.request\n",
    "dataset_path = os.path.join(WORK_FOLDER, DATASET_FILENAME)\n",
    "if not os.path.exists(dataset_path):\n",
    "    urllib.request.urlretrieve(DATASET_URL, dataset_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded dataset of size (1000000, 128); metric: 'euclidean'.\n",
      "Number of test queries: 10000\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "f = h5py.File(dataset_path, \"r\")\n",
    "\n",
    "metric = f.attrs['distance']\n",
    "\n",
    "dataset = cp.array(f['train'])\n",
    "queries = cp.array(f['test'])\n",
    "gt_neighbors = cp.array(f['neighbors'])\n",
    "gt_distances = cp.array(f['distances'])\n",
    "\n",
    "print(f\"Loaded dataset of size {dataset.shape}; metric: '{metric}'.\")\n",
    "print(f\"Number of test queries: {queries.shape[0]}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the index\n",
    "Construction of the index generally consists of two phases: training (building the clusters) and filling-in (extending the index with data).\n",
    "In the first phase, a balanced hierarchical k-means algorithm clusters the training data.\n",
    "In the second phase, the new data is classified and added into the appropriate clusters in the index.\n",
    "Hence, a user should call `ivf_pq.build` once and then possibly `ivf_pq.extend` several times.\n",
    "Though for user convenience `ivf_pq.build` by default adds the whole training set into the index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RAFT's DeviceResources controls the GPU, cuda stream, memory policies etc.\n",
    "# For now, we just create a default instance.\n",
    "from pylibraft.common import DeviceResources\n",
    "resources = DeviceResources()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pylibraft.neighbors import ivf_pq\n",
    "# First, we need to initialize the build/indexing parameters.\n",
    "# One of the more important parameters is the product quantisation (PQ) dim.\n",
    "# Effectively, this parameter says\n",
    "#      \"shrink the dataset to this dimensionality to reduce the index size\".\n",
    "# It must be not bigger than the dataset dim,\n",
    "# and it should be divisible by 32 for better GPU performance.\n",
    "pq_dim = 1\n",
    "while pq_dim * 2 < dataset.shape[1]:\n",
    "    pq_dim = pq_dim * 2\n",
    "# We'll use the ANN-benchmarks-provided metric and sensible defaults for the rest of parameters.\n",
    "index_params = ivf_pq.IndexParams(n_lists=1024, metric=metric, pq_dim=pq_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.89 s, sys: 752 ms, total: 4.64 s\n",
      "Wall time: 4.65 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Index(type=IVF-PQ, metric=euclidean, codebook=subspace, size=1000000, dim=128, pq_dim=64, pq_bits=8, n_lists=1024, rot_dim=128)"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "## Build the index\n",
    "# This function takes a row-major either numpy or cupy (GPU) array.\n",
    "# Generally, it's a bit faster with GPU inputs, but the CPU version may come in handy\n",
    "# if the whole dataset cannot fit into GPU memory (or even CPU RAM -- use mmap for that).\n",
    "index = ivf_pq.build(index_params, dataset, handle=resources)\n",
    "index"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Search\n",
    "The search function returns the requested number `k` of (approximate) nearest neighbor in no particular order.\n",
    "Besides the queries and `k`, the function can take a few more parameters to tweak the performance of the algorithm.\n",
    "Again, these are passed via the struct with some sensible defaults."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 10\n",
    "search_params = ivf_pq.SearchParams()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 41.7 ms, sys: 1.73 ms, total: 43.4 ms\n",
      "Wall time: 43.1 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "distances, neighbors = ivf_pq.search(search_params, index, queries, k, handle=resources)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's get the results back to CPU.\n",
    "# pylibraft functions are often asynchronous so the GPU needs to be explicitly synchronized\n",
    "distances = cp.asarray(distances)\n",
    "neighbors = cp.asarray(neighbors)\n",
    "resources.sync()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got recall = 0.85227 with the default parameters (k = 10).\n"
     ]
    }
   ],
   "source": [
    "## Check the quality of the prediction (recall)\n",
    "def calc_recall(found_indicies, ground_truth):\n",
    "    found_indicies = cp.asarray(found_indicies)\n",
    "    bs, k = found_indicies.shape\n",
    "    if bs != ground_truth.shape[0]:\n",
    "        raise RuntimeError(\n",
    "            \"Batch sizes do not match {} vs {}\".format(\n",
    "                bs, ground_truth.shape[0])\n",
    "        )\n",
    "    if k > ground_truth.shape[1]:\n",
    "        raise RuntimeError(\n",
    "            \"Not enough indicies in the ground truth ({} > {})\".format(\n",
    "                k, ground_truth.shape[1])\n",
    "        )\n",
    "    n = 0\n",
    "    # Go over the batch\n",
    "    for i in range(bs):\n",
    "        # Note, ivf-pq does not guarantee the ordered input, hence the use of intersect1d\n",
    "        n += cp.intersect1d(found_indicies[i, :k], ground_truth[i, :k]).size\n",
    "    recall = n / found_indicies.size\n",
    "    return recall\n",
    "\n",
    "recall_first_try = calc_recall(neighbors, gt_neighbors)\n",
    "print(f\"Got recall = {recall_first_try} with the default parameters (k = {k}).\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
